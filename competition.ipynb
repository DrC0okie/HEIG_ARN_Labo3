{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_mice = pd.read_csv('./pw3_data/EEG_mouse_data_1.csv')\n",
    "training_mice2 = pd.read_csv('./pw3_data/EEG_mouse_data_2.csv')\n",
    "##testing_mice = pd.read_csv('./pw3_data/EEG_mouse_data_test.csv')\n",
    "\n",
    "training_data = pd.concat([training_mice, training_mice2], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing features\n",
    "\n",
    "First idea use ANOVA correlation coefficient for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "# first_25 = [\n",
    "#     \"state\",\n",
    "#     \"amplitude_around_1_Hertz\",\n",
    "#     \"amplitude_around_2_Hertz\",\n",
    "#     \"amplitude_around_3_Hertz\",\n",
    "#     \"amplitude_around_4_Hertz\",\n",
    "#     \"amplitude_around_5_Hertz\",\n",
    "#     \"amplitude_around_6_Hertz\",\n",
    "#     \"amplitude_around_7_Hertz\",\n",
    "#     \"amplitude_around_8_Hertz\",\n",
    "#     \"amplitude_around_9_Hertz\",\n",
    "#     \"amplitude_around_10_Hertz\",\n",
    "#     \"amplitude_around_11_Hertz\",\n",
    "#     \"amplitude_around_12_Hertz\",\n",
    "#     \"amplitude_around_13_Hertz\",\n",
    "#     \"amplitude_around_14_Hertz\",\n",
    "#     \"amplitude_around_15_Hertz\",\n",
    "#     \"amplitude_around_16_Hertz\",\n",
    "#     \"amplitude_around_17_Hertz\",\n",
    "#     \"amplitude_around_18_Hertz\",\n",
    "#     \"amplitude_around_19_Hertz\",\n",
    "#     \"amplitude_around_20_Hertz\",\n",
    "#     \"amplitude_around_21_Hertz\",\n",
    "#     \"amplitude_around_22_Hertz\",\n",
    "#     \"amplitude_around_23_Hertz\",\n",
    "#     \"amplitude_around_24_Hertz\",\n",
    "#     \"amplitude_around_25_Hertz\",\n",
    "# ]\n",
    "\n",
    "target = training_data.iloc[:, 0]  # Assuming the first column is the class\n",
    "features = training_data.iloc[:, 1:]  # Rest of the columns are features\n",
    "\n",
    "#Calculated the ANOVA correlation coefficient for each feature\n",
    "anova_corr_coef, _ = f_classif(features, target)\n",
    "result = pd.DataFrame({'Feature': features.columns, 'anova_coeff': anova_corr_coef})\n",
    "sorted_result = result.sort_values(by=['anova_coeff'],ascending=False)\n",
    "first_25 = sorted_result[:50][\"Feature\"].to_list()\n",
    "first_25.insert(0, \"state\")\n",
    "\n",
    "input_training_mice = training_data[first_25]\n",
    "\n",
    "\n",
    "\n",
    "print(input_training_mice.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and encode data\n",
    "\n",
    "balance data and fit and transform all column except \"state\"  with StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "encoder = OneHotEncoder(sparse_output=False).set_output(transform=\"pandas\")\n",
    "# Encode the state\n",
    "state = pd.DataFrame(input_training_mice['state'])\n",
    "encoder.fit(state)\n",
    "\n",
    "output_training_mice = encoder.transform(state)\n",
    "input_training_mice = input_training_mice.drop(columns=['state'])\n",
    "\n",
    "\n",
    "\n",
    "for column in input_training_mice:\n",
    "    column_data = input_training_mice[column].to_frame()\n",
    "    scaler.fit(column_data)\n",
    "    input_training_mice[column] = scaler.transform(column_data)\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_tuner as kt\n",
    "from keras import layers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "keras.utils.set_random_seed(123)\n",
    "kf = KFold(n_splits=3, shuffle=True)\n",
    "\n",
    "def create_model():\n",
    "\n",
    "  mlp = keras.Sequential()\n",
    "  mlp.add(layers.Input(shape=(50,)))\n",
    "  mlp.add(layers.Dense(8, activation=\"leaky_relu\"))\n",
    "  \n",
    "  mlp.add(layers.Dense(3, activation=\"softmax\"))\n",
    "  \n",
    "  learning_rate = 0.001\n",
    " \n",
    "  optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    " \n",
    "\n",
    "  mlp.compile(\n",
    "      optimizer=optimizer,\n",
    "      loss=\"categorical_crossentropy\",\n",
    "      metrics=[keras.metrics.F1Score(average=\"weighted\")]\n",
    "  )\n",
    "\n",
    "  return mlp\n",
    "\n",
    "# tuner = kt.GridSearch(\n",
    "#     create_model,\n",
    "#     objective= kt.Objective(\"val_f1_score\", direction=\"max\"),\n",
    "#     overwrite=True,\n",
    "#     directory=\"./tuning\",\n",
    "\n",
    "# )\n",
    "\n",
    "\n",
    "mlp = create_model()\n",
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Idea 2, SMOTE (Synthetic Minority Over-sampling Technique ) to balance classes observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "history_list = []\n",
    "trained_mlp = []\n",
    "smote = SMOTE()\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(input_training_mice)):\n",
    "  # We need to create a new model everytime otherwise fit will continue previous training\n",
    " \n",
    "  \n",
    "  x_train, x_test = input_training_mice.iloc[train_index], input_training_mice.iloc[test_index]\n",
    "  y_train, y_test = np.array(output_training_mice)[train_index],np.array(output_training_mice)[test_index]\n",
    "  \n",
    "  # if(i == 0):\n",
    "  #   tuner.search(\n",
    "  #     x=x_train, y=y_train,\n",
    "  #     validation_data=(x_test, y_test),\n",
    "  #     epochs=100,\n",
    "  #     callbacks=[keras.callbacks.TensorBoard(\"./tmp/tb_logs\")]\n",
    "  #   )\n",
    "  #   tuner.results_summary()\n",
    "  # else:\n",
    "  #best_hps = tuner.get_best_hyperparameters(1)\n",
    "  mlp = create_model()\n",
    "  #x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    \n",
    "  history = mlp.fit(\n",
    "      x=x_train, y=y_train,\n",
    "      validation_data=(x_test, y_test),\n",
    "      epochs=100\n",
    "  )\n",
    "  history_list.append(history)\n",
    "  trained_mlp.append(mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pl\n",
    "%matplotlib inline\n",
    "\n",
    "train_losses = np.array([history.history['loss'] for history in history_list])\n",
    "val_losses = np.array([history.history['val_loss'] for history in history_list])\n",
    "\n",
    "# Calculate mean and standard deviation for training and validation losses\n",
    "mean_train_loss = np.mean(train_losses, axis=0)\n",
    "std_train_loss = np.std(train_losses, axis=0)\n",
    "mean_val_loss = np.mean(val_losses, axis=0)\n",
    "std_val_loss = np.std(val_losses, axis=0)\n",
    "\n",
    "# Plot mean and standard deviation for training loss\n",
    "pl.plot(mean_train_loss, label='Training Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_train_loss)), mean_train_loss - std_train_loss, mean_train_loss + std_train_loss, alpha=0.3, label='Training Loss (Std)')\n",
    "\n",
    "# Plot mean and standard deviation for validation loss\n",
    "pl.plot(mean_val_loss, label='Validation Loss (Mean)')\n",
    "pl.fill_between(range(len(mean_val_loss)), mean_val_loss - std_val_loss, mean_val_loss + std_val_loss, alpha=0.3, label='Validation Loss (Std)')\n",
    "\n",
    "# Add labels and legend\n",
    "pl.xlabel('Epochs')\n",
    "pl.ylabel('Loss')\n",
    "pl.legend()\n",
    "\n",
    "# Display the plot\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix, title):\n",
    "    # Plot confusion matrix\n",
    "    pl.figure(figsize=(8, 6))\n",
    "    sns.heatmap(confusion_matrix.astype(int), annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "                xticklabels=[\"rem\",\"n-rem\",\"awake\"], yticklabels=[\"rem\",\"n-rem\", \"awake\"])\n",
    "    pl.title(title)\n",
    "    pl.xlabel('Predicted')\n",
    "    pl.ylabel('True')\n",
    "    pl.show()\n",
    "\n",
    "f1_scores = []\n",
    "mean_confusion_matrix = np.zeros((3, 3))\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(input_training_mice)):\n",
    "    # Evaluate the trained model on the test fold\n",
    "    predictions = trained_mlp[i-1].predict(input_training_mice.loc[test_index])\n",
    "    true_labels = np.array(output_training_mice)[test_index]\n",
    "    \n",
    "    max_predictions = []\n",
    "    max_true_labels = []\n",
    "    for prediction in predictions:\n",
    "        max_predictions.append(np.argmax(prediction))\n",
    "        \n",
    "    for true_label in true_labels:\n",
    "        max_true_labels.append(np.argmax(true_label))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(max_true_labels, max_predictions)\n",
    "    mean_confusion_matrix += confusion_matrix(max_true_labels, max_predictions)\n",
    "\n",
    "    # Compute confusion matrix and plot\n",
    "    plot_confusion_matrix(cm, f'Confusion Matrix - Fold {i + 1}')\n",
    "\n",
    "    # Compute F1 score\n",
    "    f1 = f1_score(max_true_labels, max_predictions, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    print(f\"F1 Score - Fold {i + 1}: {f1}\")\n",
    "\n",
    "# Plot mean confusion matrix\n",
    "plot_confusion_matrix(mean_confusion_matrix, 'Global confusion matrix')\n",
    "\n",
    "# Calculate and display the mean F1 score across all folds\n",
    "mean_f1_score = np.mean(f1_scores)\n",
    "print(f\"Mean F1 Score across all folds: {mean_f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Résultat\n",
    "\n",
    "Voir rapport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
